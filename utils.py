import os
import glob
import json
import time
import requests
import re
from dateutil import parser
from datetime import datetime
import pandas as pd
from google.oauth2.service_account import Credentials

from googleapiclient.discovery import build
from dotenv import load_dotenv
from google.genai import Client
from google.genai.types import GenerateContentConfig
load_dotenv()

FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

client = Client(api_key=GOOGLE_API_KEY)

RESULTS_DIR = "results"
DOCUMENT_ID = "19S3OprOCXXxmo8FjkivBtz_t2t5isenYg-AVqqzA2-U"
creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
SPREADSHEET_ID = "1snMFj6e4X3YUK_48xXJlb8VhLwcS4vSxb69LgoBcDO4"

# H√†m l·ªçc tr√πng c√°c b√†i b√°o t√¨m ƒë∆∞·ª£c v√† chu·∫©n h√≥a Doi -> Link
def remove_duplicates_and_convert_doi(papers):
    unique_papers = []
    seen_dois = set()
    seen_titles = set()

    for paper in papers:
        doi = paper.get("doi")
        title = paper.get("title", "").lower().strip()

        authors = paper.get("authors", [])
        if isinstance(authors, list):
            paper["authors"] = ", ".join(authors) if authors else "Not Available"

        if doi:
            doi_link = doi if doi.startswith("http") else f"https://doi.org/{doi}"
            if doi not in seen_dois:
                paper["doi"] = doi_link  
                unique_papers.append(paper)
                seen_dois.add(doi)
        elif title and title not in seen_titles:
            unique_papers.append(paper)
            seen_titles.add(title)

    return unique_papers


# H√†m l·∫•y file trong results
def get_latest_json():
    """
    L·∫•y file JSON m·ªõi nh·∫•t theo ng√†y c√≥ d·∫°ng: YYYY-MM-DD_search_paper.json
    """
    pattern = os.path.join(RESULTS_DIR, "*_search_paper.json")
    json_files = glob.glob(pattern)

    if not json_files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON n√†o trong th∆∞ m·ª•c results/")
        return None

    # L·∫•y ng√†y t·ª´ t√™n file v√† ch·ªçn ng√†y m·ªõi nh·∫•t
    files_with_dates = []
    for f in json_files:
        base = os.path.basename(f)
        try:
            date_part = base.split("_")[0]  # L·∫•y ph·∫ßn YYYY-MM-DD
            datetime.strptime(date_part, "%Y-%m-%d")  # ki·ªÉm tra format
            files_with_dates.append((date_part, f))
        except Exception:
            continue

    if not files_with_dates:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON h·ª£p l·ªá theo ng√†y")
        return None

    # Ch·ªçn file c√≥ ng√†y m·ªõi nh·∫•t
    latest_file = max(files_with_dates, key=lambda x: x[0])[1]
    print(f"üìÇ File JSON m·ªõi nh·∫•t theo ng√†y: {latest_file}")
    return latest_file

# H√†m l∆∞u file v√†o results
def save_results_to_json(data, output_dir=RESULTS_DIR, prefix="search_paper"):
    """
    L∆∞u k·∫øt qu·∫£ v√†o file JSON v·ªõi t√™n ch·ª©a timestamp.
    N·∫øu c√πng 1 ng√†y ƒë√£ c√≥ file -> load d·ªØ li·ªáu c≈©, merge th√™m d·ªØ li·ªáu m·ªõi (l·ªçc tr√πng theo DOI ho·∫∑c title), r·ªìi ghi ƒë√® l·∫°i.
    """
    os.makedirs(output_dir, exist_ok=True)
    today_str = datetime.now().strftime("%Y-%m-%d")

    # --- 1Ô∏è‚É£ T√¨m file trong ng√†y h√¥m nay ---
    existing_file = None
    for fname in os.listdir(output_dir):
        if fname.startswith(today_str) and fname.endswith(f"{prefix}.json"):
            existing_file = os.path.join(output_dir, fname)
            break

    # --- 2Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu c≈© n·∫øu c√≥ ---
    merged_data = []
    if existing_file:
        try:
            with open(existing_file, "r", encoding="utf-8") as f:
                old_data = json.load(f)
            merged_data = old_data
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc file c≈© {existing_file}: {e}")
            merged_data = []
    else:
        filename = f"{today_str}_{prefix}.json"
        existing_file = os.path.join(output_dir, filename)

    # --- 3Ô∏è‚É£ T·∫°o set ch·ª©a DOI ho·∫∑c title ƒë·ªÉ ki·ªÉm tra tr√πng ---
    existing_keys = set()
    for item in merged_data:
        key = (item.get("doi") or item.get("title") or "").strip().lower()
        if key:
            existing_keys.add(key)

    # --- 4Ô∏è‚É£ L·ªçc c√°c b√†i m·ªõi ch∆∞a c√≥ trong existing_keys ---
    new_filtered = []
    for p in data:
        key = (p.get("doi") or p.get("title") or "").strip().lower()
        if key and key not in existing_keys:
            new_filtered.append(p)
            existing_keys.add(key)

    # --- 5Ô∏è‚É£ N·∫øu kh√¥ng c√≥ b√†i m·ªõi th√¨ b·ªè qua ---
    if not new_filtered:
        print("‚è© Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ th√™m.")
        return existing_file

    # --- 6Ô∏è‚É£ G·ªôp v√† ghi l·∫°i ---
    merged_data.extend(new_filtered)
    try:
        with open(existing_file, "w", encoding="utf-8") as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        print(f"üíæ ƒê√£ c·∫≠p nh·∫≠t file: {existing_file} (th√™m {len(new_filtered)} b√†i b√°o m·ªõi)")
        return existing_file
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u file JSON: {e}")
        return None


# H√†m ƒë·ªãnh nghƒ©a GG Docx
def get_creds():
    scopes = ["https://www.googleapis.com/auth/documents"]

    # D√†nh cho GitHub Actions
    creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if creds_path and os.path.exists(creds_path):
        return Credentials.from_service_account_file(creds_path, scopes=scopes)

    # D√†nh cho ch·∫°y local
    local_path = r"D:\GitHub\Key_gg_sheet\eternal-dynamo-474316-f6-382e31e4ae72.json"
    if os.path.exists(local_path):
        return Credentials.from_service_account_file(local_path, scopes=scopes)

    raise FileNotFoundError("‚ùå Kh√¥ng t√¨m th·∫•y file Google credential n√†o h·ª£p l·ªá.")

# H√†m th·ª±c hi·ªán ghi d·ªØ li·ªáu l√™n GG Docx
def append_json_to_gdoc(df, date_str):
    creds = get_creds()
    service = build("docs", "v1", credentials=creds)
    doc = service.documents().get(documentId=DOCUMENT_ID).execute()
    content = doc.get('body', {}).get('content', [])

    # --- 1Ô∏è‚É£ T√¨m v·ªã tr√≠ c·ªßa ph·∫ßn "Ng√†y <date_str>" ---
    target_header = f"Ng√†y {date_str}"
    today_start = today_end = None

    for i, element in enumerate(content):
        text_elements = element.get("paragraph", {}).get("elements", [])
        if not text_elements:
            continue
        text = "".join([e.get("textRun", {}).get("content", "") for e in text_elements])
        if target_header.strip() in text.strip():
            today_start = element.get("startIndex", 1)
            # t√¨m endIndex c·ªßa ph·∫ßn h√¥m nay = v·ªã tr√≠ "Ng√†y " k·∫ø ti·∫øp
            for j in range(i + 1, len(content)):
                next_text_elements = content[j].get("paragraph", {}).get("elements", [])
                next_text = "".join([e.get("textRun", {}).get("content", "") for e in next_text_elements])
                if next_text.startswith("Ng√†y "):  # ph·∫ßn ng√†y kh√°c (h√¥m sau ho·∫∑c h√¥m qua)
                    today_end = content[j].get("startIndex", content[j].get("endIndex"))
                    break
            # n·∫øu kh√¥ng c√≥ ng√†y k·∫ø ti·∫øp ‚Üí ph·∫ßn n√†y k√©o t·ªõi cu·ªëi file
            if not today_end:
                today_end = content[-1].get("endIndex")
            break

    requests = []

    # --- 2Ô∏è‚É£ N·∫øu t√¨m th·∫•y "Ng√†y h√¥m nay" ‚Üí x√≥a ---
    if today_start and today_end:
        print(f"üóëÔ∏è ƒê√£ t√¨m th·∫•y 'Ng√†y {date_str}', ti·∫øn h√†nh ghi ƒë√®.")
        delete_request = [{
            "deleteContentRange": {
                "range": {"startIndex": today_start, "endIndex": today_end - 1}
            }
        }]
        service.documents().batchUpdate(documentId=DOCUMENT_ID, body={"requests": delete_request}).execute()
        insert_index = today_start
    else:
        # n·∫øu ch∆∞a c√≥ ng√†y h√¥m nay ‚Üí th√™m m·ªõi ·ªü cu·ªëi
        insert_index = content[-1].get("endIndex", 1) - 1
        print(f"üÜï Kh√¥ng t√¨m th·∫•y 'Ng√†y {date_str}', th√™m m·ªõi v√†o cu·ªëi t√†i li·ªáu.")

    # --- 3Ô∏è‚É£ T·∫°o ti√™u ƒë·ªÅ ng√†y ---
    day_header = f"\nNg√†y {date_str}\n\n"
    requests.append({
        "insertText": {"location": {"index": insert_index}, "text": day_header}
    })
    requests.append({
        "updateTextStyle": {
            "range": {"startIndex": insert_index, "endIndex": insert_index + len(day_header)},
            "textStyle": {
                "bold": True,
                "foregroundColor": {"color": {"rgbColor": {"red": 1, "green": 0, "blue": 0}}}
            },
            "fields": "bold,foregroundColor"
        }
    })

    current_index = insert_index + len(day_header)

    # --- 4Ô∏è‚É£ Ghi n·ªôi dung t·ª´ng b√†i b√°o ---
    for i, row in df.iterrows():
        title = row.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
        authors = row.get("authors", "Kh√¥ng r√µ t√°c gi·∫£")
        pubdate = row.get("pubdate", "Kh√¥ng r√µ ng√†y")
        innovative = row.get("innovative", "").strip()
        journal = row.get("journal", "Kh√¥ng r√µ t·∫°p ch√≠")
        doi = row.get("doi", "")
        link = row.get("link", "")
        hyperlink = doi if doi else link

        text_block = (
            f"{i+1}. {title}\n"
            f"T√°c gi·∫£: {authors}\n"
            f"T·∫°p ch√≠: {journal}\n"
            f"Ng√†y xu·∫•t b·∫£n: {pubdate}\n"
            f"Innovative: {innovative}\n"
        )

        # th√™m m√¥ t·∫£
        requests.append({
            "insertText": {"location": {"index": current_index}, "text": text_block}
        })

        title_start = current_index + len(f"{i+1}. ")
        title_end = title_start + len(title)

        # l√†m ƒë·∫≠m ti√™u ƒë·ªÅ
        requests.append({
            "updateTextStyle": {
                "range": {"startIndex": title_start, "endIndex": title_end},
                "textStyle": {"bold": True},
                "fields": "bold"
            }
        })

        current_index += len(text_block)

        # th√™m ‚ÄúXem b√†i b√°o‚Äù n·∫øu c√≥ link
        if hyperlink:
            link_text = "Xem b√†i b√°o\n\n"
            requests.append({
                "insertText": {"location": {"index": current_index}, "text": link_text}
            })
            requests.append({
                "updateTextStyle": {
                    "range": {
                        "startIndex": current_index,
                        "endIndex": current_index + len("Xem b√†i b√°o")
                    },
                    "textStyle": {
                        "link": {"url": hyperlink},
                        "bold": True,
                        "foregroundColor": {"color": {"rgbColor": {"blue": 1.0}}}
                    },
                    "fields": "link,bold,foregroundColor"
                }
            })
            current_index += len(link_text)
        else:
            requests.append({
                "insertText": {"location": {"index": current_index}, "text": "\n"}
            })
            current_index += 1

    # --- 5Ô∏è‚É£ G·ª≠i to√†n b·ªô request c·∫≠p nh·∫≠t ---
    service.documents().batchUpdate(documentId=DOCUMENT_ID, body={"requests": requests}).execute()

    print(f"‚úÖ ƒê√£ ghi ƒë√® n·ªôi dung c·ªßa ng√†y {date_str} (c√°c ng√†y kh√°c ƒë∆∞·ª£c gi·ªØ nguy√™n).")


# H√†m ghi d·ªØ li·ªáu l√™n GG Docx
def convert_latest_json_to_gdoc():
    latest_file = get_latest_json()  
    if not latest_file:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON.")
        return

    today_str = datetime.now().strftime("%Y-%m-%d")
    base = os.path.basename(latest_file)
    file_date = base.split("_")[0]

    if file_date != today_str:
        print("‚ÑπÔ∏è File JSON m·ªõi nh·∫•t kh√¥ng ph·∫£i c·ªßa h√¥m nay.")
        return

    with open(latest_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    df = pd.DataFrame(data)
    append_json_to_gdoc(df, today_str)


# H√†m th·ª±c hi·ªán l·∫•y Abstract v√† Pubdate 
def fetch_abstract_and_pubdate_firecrawl(url):
    if not FIRECRAWL_API_KEY:
        raise ValueError("Thi·∫øu FIRECRAWL_API_KEY, h√£y set trong bi·∫øn m√¥i tr∆∞·ªùng.")

    api_url = "https://api.firecrawl.dev/v1/scrape"
    headers = {"Authorization": f"Bearer {FIRECRAWL_API_KEY}"}
    payload = {
        "url": url,
        "formats": ["markdown"],  
    }

    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=60)
        resp.raise_for_status()
        data = resp.json()
    except requests.exceptions.RequestException as e:
        print(f"[Firecrawl Error] {e}")
        return {"abstract": "Not Available", "pubdate": "Not Available"}

    content = data.get("data", {}).get("markdown", "")
    if not content:
        return {"abstract": "Not Available", "pubdate": "Not Available"}

    lines = content.splitlines()
    abstract_lines = []
    capture = False

    for line in lines:
        low = line.lower().strip()
        # B·∫Øt ƒë·∫ßu t·ª´ Abstract / T√≥m t·∫Øt
        if "abstract" in low or "t√≥m t·∫Øt" in low:
            capture = True
            continue
        # N·∫øu g·∫∑p Keywords / Introduction th√¨ d·ª´ng l·∫°i
        if capture and ("keywords" in low or "introduction" in low or "references" in low):
            break
        if capture:
            abstract_lines.append(line.strip())

    abstract = " ".join(abstract_lines).strip()
    
    # --- Tr√≠ch xu·∫•t pubdate t·ª´ markdown ---
    pubdate = "Not Available"
    # t√¨m c√°c m·∫´u nh∆∞ "Published: 2025-01-20" ho·∫∑c "Ng√†y xu·∫•t b·∫£n: 20 Jan 2025"
    date_patterns = [
        r'published[:\s]+(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
        r'published[:\s]+(\d{1,2}\s\w+\s\d{4})', # DD Month YYYY
        r'ng√†y xu·∫•t b·∫£n[:\s]+(\d{1,2}\s\w+\s\d{4})',
    ]
    for pattern in date_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            try:
                pubdate = str(parser.parse(match.group(1)).date())
                break
            except:
                continue
    return {"abstract": abstract, "pubdate": pubdate}


# H√†m l·∫•y Abstract v√† Pubdate 
def enrich_with_firecrawl(results):
    for paper in results:
        needs_fetch = (
            (not paper.get("abstract") or paper["abstract"] == "Not Available") or
            (not paper.get("pubdate") or paper["pubdate"] == "Not Available")
        )
        url = paper.get("link") or paper.get("doi") or paper.get("landing_page") or "Not Available"
        if needs_fetch and url != "Not Available":
            print(f"Fetching abstract & pubdate with Firecrawl for: {paper['title']}")
            data = fetch_abstract_and_pubdate_firecrawl(url)
            paper["abstract"] = data["abstract"]
            paper["pubdate"] = data["pubdate"]
            time.sleep(6)
    return results


# H√†m th·ª±c hi·ªán ƒë√°nh gi√° li√™n quan
def evaluate_paper_relevance(abstract, keywords):
    prompt = f"""
    You are a senior researcher specialized in Non-Destructive Testing (NDT), Pulsed Eddy Current (PEC), 
    and related electromagnetic or signal processing methods in engineering.

    Task:
    1. Carefully read the abstract below.
    2. Think step by step (silently) about whether the abstract‚Äôs main topic, method, or application 
    is conceptually or methodologically related to any of the following:
    - {", ".join(keywords)}
    - Non-Destructive Testing (NDT)
    - Pulsed Eddy Current (PEC)
    3. If there is at least one clear or close relation, answer "YES". 
    If there is no clear connection at all, answer "NO".
    4. Double-check your decision consistency before finalizing.

    IMPORTANT:
    - Think internally, but output ONLY one word: YES or NO.
    - Do not provide explanation or reasoning.

    Example guidance:
    Abstract: "This paper applies pulsed eddy current sensing to detect corrosion in pipelines."
    ‚Üí Output: YES

    Abstract: "This study uses seismic inversion for subsurface mapping."
    ‚Üí Output: NO

    Now analyze the following abstract:
    {abstract}
    """


    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=GenerateContentConfig(temperature=0)
        )

        text = response.text.strip().upper()
        related = "YES" in text
        return {"related": related}

    except Exception as e:
        print(f"[Gemini Error - Relevance Evaluation] {e}")
        return {"related": False}



# H√†m l·ªçc c√°c b√†i b√°o kh√¥ng li√™n quan li√™n quan
def filter_relevant_papers(results, keywords):
    relevant_papers = []

    for paper in results:
        title = paper.get("title", "Untitled")
        abstract = paper.get("abstract", "").strip()

        if not abstract or abstract.lower() == "not available":
            continue

        print(f"Checking relevance for: {title}")
        evaluation = evaluate_paper_relevance(abstract, keywords)

        if evaluation["related"]:
            relevant_papers.append(paper)
        else:
            print(f"‚ùå Paper '{title}' is not relevant.")

        time.sleep(4)  

    return relevant_papers


# H√†m l·∫•y ranking c·ªßa journal 
def get_journal_rank(journal_name):
    if not journal_name:
        return None
    try:
        url = f"https://api.openalex.org/sources?search={journal_name}"
        r = requests.get(url, timeout=10)
        data = r.json()
        if "results" in data and data["results"]:
            return data["results"][0].get("best_quartile", "Unknown")
    except Exception as e:
        print(f"[OpenAlex Error] {e}")
    return None


# H√†m l·∫•y h-index c·ªßa author
def get_author_hindex(author_name):
    try:
        url = f"https://api.semanticscholar.org/graph/v1/author/search?query={author_name}&limit=1"
        r = requests.get(url, timeout=10)
        data = r.json()
        if "data" in data and data["data"]:
            author_id = data["data"][0]["authorId"]
            detail = requests.get(
                f"https://api.semanticscholar.org/graph/v1/author/{author_id}?fields=hIndex",
                timeout=10
            ).json()
            return detail.get("hIndex")
    except Exception as e:
        print(f"[SemanticScholar Error] {e}")
    return None


# H√†m ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng b√†i b√°o, jounal v√† arthur b·∫±ng llm
def evaluate_paper_quality_llm(abstract):
    prompt = f"""
    You are a senior reviewer for high-impact engineering journals.

    Read the following abstract and rate its RESEARCH QUALITY from 0‚Äì100,
    based on:
    - Novelty and originality of the idea.
    - Technical depth and clarity of methods.
    - Contribution and scientific value.
    - Writing clarity and coherence.

    Output strictly as: "SCORE: <number>"

    Abstract:
    {abstract}
    """
    time.sleep(4)
    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=GenerateContentConfig(temperature=0)
        )
        text = response.text.strip().upper()
        match = re.search(r"SCORE[:\s]*([0-9]+)", text)
        return int(match.group(1)) if match else 50
    except Exception as e:
        print(f"[Gemini Error - LLM Quality] {e}")
        return 50
    


# H√†m ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng
def evaluate_paper_quality(article, keywords):
    """
    ƒê√°nh gi√° t·ªïng th·ªÉ ch·∫•t l∆∞·ª£ng b√†i b√°o g·ªìm:
    - Journal rank (25%)
    - Author h-index (25%)
    - LLM ƒë√°nh gi√° n·ªôi dung (50%)
    """

    if isinstance(article, list):
        return [evaluate_paper_quality(a, keywords) for a in article]

    title = article.get("title", "Untitled")
    abstract = article.get("abstract", "").strip()
    authors = article.get("authors", "Not Available")
    journal = article.get("journal", "")

    # ‚ö†Ô∏è Kh√¥ng c√≥ abstract
    if not abstract or abstract.lower() == "not available":
        article.update({
            "related": False,
            "score": 0,
            "evaluation": "‚ùå Kh√¥ng c√≥ abstract ƒë·ªÉ ƒë√°nh gi√°.",
            "quality_level": "‚ö†Ô∏è Thi·∫øu d·ªØ li·ªáu"
        })
        return article

    comments = []

    # --- 2Ô∏è‚É£ Journal (25%) ---
    journal_rank = get_journal_rank(journal)
    journal_score_map = {
        "Q1": 100, "Q2": 80, "Q3": 60, "Q4": 40,
        "A*": 100, "A": 90, "B": 70, "C": 50
    }
    raw_journal_score = journal_score_map.get(journal_rank, 30)
    journal_score = raw_journal_score * 0.25 / 100 * 100  # quy ƒë·ªïi ra %
    comments.append(f"üè´ Journal '{journal}' rank {journal_rank or 'Unknown'} (+{journal_score:.1f} ƒëi·ªÉm).")

    # --- 3Ô∏è‚É£ Author (25%) ---
    if authors == "Not Available":
        author_score = 0
        comments.append("üë§ Kh√¥ng c√≥ th√¥ng tin t√°c gi·∫£ (0 ƒëi·ªÉm).")
    else:
        first_author = authors.split(",")[0].strip()
        h_index = get_author_hindex(first_author)
        if h_index is None:
            author_score = 5
            comments.append(f"üë§ Kh√¥ng l·∫•y ƒë∆∞·ª£c h-index c·ªßa {first_author} (+5 ƒëi·ªÉm t·∫°m).")
        elif h_index >= 40:
            author_score = 25
            comments.append(f"üë§ {first_author} c√≥ h-index cao ({h_index}) (+25 ƒëi·ªÉm).")
        elif h_index >= 20:
            author_score = 18
            comments.append(f"üë§ {first_author} c√≥ h-index trung b√¨nh ({h_index}) (+18 ƒëi·ªÉm).")
        else:
            author_score = 10
            comments.append(f"üë§ {first_author} c√≥ h-index th·∫•p ({h_index}) (+10 ƒëi·ªÉm).")

    # --- 4Ô∏è‚É£ LLM (50%) ---
    llm_raw = evaluate_paper_quality_llm(abstract)
    llm_score = llm_raw * 0.5 / 100 * 100
    comments.append(f"ü§ñ LLM ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng abstract {llm_raw}/100 (+{llm_score:.1f} ƒëi·ªÉm).")

    # --- 5Ô∏è‚É£ T·ªïng ƒëi·ªÉm ---
    total_score = round(journal_score + author_score + llm_score, 2)
    total_score = min(total_score, 100)

    # --- 6Ô∏è‚É£ X·∫øp lo·∫°i ---
    quality_level = (
        "üèÖ Xu·∫•t s·∫Øc" if total_score >= 85 else
        "üëç T·ªët" if total_score >= 70 else
        "‚öñÔ∏è Trung b√¨nh" if total_score >= 50 else
        "‚ö†Ô∏è Y·∫øu"
    )

    # --- 7Ô∏è‚É£ C·∫≠p nh·∫≠t k·∫øt qu·∫£ ---
    article.update({
        "related": True,
        "journal_rank": journal_rank or "Unknown",
        "score": total_score,
        "evaluation": " | ".join(comments),
        "quality_level": quality_level
    })

    return article



# H√†m l·∫•y top c√°c b√†i b√°o ƒë∆∞·ª£c ƒë√°nh gi√° hay nh·∫•t
def get_top_quality_papers(evaluated_papers, top_n=10):
    # Ch·ªâ l·∫•y c√°c b√†i li√™n quan v√† c√≥ ƒëi·ªÉm > 0
    valid_papers = [p for p in evaluated_papers if p.get("related") and p.get("score", 0) > 0]

    # S·∫Øp x·∫øp theo ƒëi·ªÉm s·ªë gi·∫£m d·∫ßn
    sorted_papers = sorted(valid_papers, key=lambda x: x.get("score", 0), reverse=True)

    # L·∫•y top N b√†i
    top_papers = sorted_papers[:top_n]

    return top_papers


# H√†m th·ª±c hi·ªán ph√¢n t√≠ch v√† t√¨m ƒëi·ªÉm s√°ng t·∫°o c·ªßa b√†i b√°o
def innovative_with_genai(abstract):
    prompt = f"""
    You are an experienced academic reviewer evaluating research abstracts.
    Your task is to identify the methodological innovation in the study described below.

    Follow this reasoning process silently before answering:

    Understand what general topic or goal the research addresses (but do not include this in the answer).

    Examine what method, technique, framework, model, or experimental process is used.

    Compare it mentally with common or traditional approaches in the same field.

    Identify what is new, improved, or distinctive about the method or process.

    Then, write the final output clearly and concisely:

    Focus only on the methodological innovation (not findings or topic).

    Write 2‚Äì4 sentences in simple academic English.

    Avoid vague terms like ‚Äúnovel,‚Äù ‚Äúinnovative,‚Äù or ‚Äúunique‚Äù unless you explain how.

    Abstract:
    {abstract}
    """

    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=GenerateContentConfig(temperature=0.3)
        )
        return response.text.strip()
    except Exception as e:
        print(f"[Gemini Error - Innovation] {e}")
        return "T√¨m ƒëi·ªÉm s√°ng t·∫°o v·ªÅ ph∆∞∆°ng ph√°p kh√¥ng th√†nh c√¥ng"



# H√†m ph√¢n t√≠ch v√† t√¨m ƒëi·ªÉm s√°ng t·∫°o c·ªßa b√†i b√°o
def innovative_filtered_papers(filtered_papers):
    for paper in filtered_papers:
        abstract = paper.get("abstract", "").strip()
        title = paper.get("title", "Untitled")
        
        if abstract:
            print(f"Innovating for: {title}")
            paper["innovative"] = innovative_with_genai(abstract)
            time.sleep(8)  

    return filtered_papers
